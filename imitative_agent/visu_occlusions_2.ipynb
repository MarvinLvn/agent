{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa1f0aae-0ad2-4a21-b363-e92dc7a1c0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as ipw\n",
    "import numpy as np \n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from imitative_agent import ImitativeAgent\n",
    "from lib.dataset_wrapper import Dataset\n",
    "from lib import utils\n",
    "from lib import abx_utils\n",
    "from lib import notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f5e19cb-778d-4ddb-b31d-d67db98a1bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents_path = glob(\"../out/imitative_agent/*/\")\n",
    "agents_path.sort()\n",
    "\n",
    "agents_alias = {}\n",
    "agents_group = {}\n",
    "\n",
    "for agent_path in agents_path:\n",
    "    agent = ImitativeAgent.reload(agent_path, load_nn=False)\n",
    "    config = agent.config\n",
    "    \n",
    "    if config[\"training\"][\"jerk_loss_ceil\"] != 0.014: continue\n",
    "        \n",
    "    agent_i = agent_path[-2]\n",
    "    agent_alias = \" \".join((\n",
    "        f\"{','.join(config['dataset']['names'])}\",\n",
    "        f\"synth_art={agent.synthesizer.config['dataset']['art_type']}\",\n",
    "        f\"jerk_c={config['training']['jerk_loss_ceil']}\",\n",
    "        f\"jerk_w={config['training']['jerk_loss_weight']}\",\n",
    "        f\"bi={config['model']['inverse_model']['bidirectional']}\",\n",
    "        f\"({agent_i})\",\n",
    "    ))\n",
    "    agents_alias[agent_alias] = agent_path\n",
    "    \n",
    "    agent_group = \" \".join((\n",
    "        f\"{','.join(config['dataset']['names'])}\",\n",
    "        f\"synth_art={agent.synthesizer.config['dataset']['art_type']}\",\n",
    "        f\"jerk_c={config['training']['jerk_loss_ceil']}\",\n",
    "        f\"jerk_w={config['training']['jerk_loss_weight']}\",\n",
    "        f\"bi={config['model']['inverse_model']['bidirectional']}\",\n",
    "    ))\n",
    "    if agent_group not in agents_group:\n",
    "        agents_group[agent_group] = []\n",
    "    agents_group[agent_group].append(agent_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46b907ea-5f85-4da3-9697-c4944563532a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TONGUE_CONSONANTS = [\"p\", \"b\", \"t\", \"d\", \"k\", \"g\"]\n",
    "DETECTION_METHODS = {\n",
    "    \"p\": \"lips\",\n",
    "    \"b\": \"lips\",\n",
    "    \"t\": \"tongue_tip\",\n",
    "    \"d\": \"tongue_tip\",\n",
    "    \"k\": \"tongue_mid\",\n",
    "    \"g\": \"tongue_mid\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31c5fb8b-4cbf-47d1-bc6d-485a00b2edfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eccef442f921473ebf43c08564aba654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e2ae29b2c1d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_main_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0magent_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat_datasplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_features\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magent_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/these/code/code-papier/lpcnet_agents/lib/base_agent.py\u001b[0m in \u001b[0;36mrepeat_datasplit\u001b[0;34m(self, datasplit_index)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mitem_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitems_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0mitem_sound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitems_sound\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0mrepetition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem_sound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mrepetition_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepetition_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrepetition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mrepetition_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/these/code/code-papier/lpcnet_agents/imitative_agent/imitative_agent.py\u001b[0m in \u001b[0;36mrepeat\u001b[0;34m(self, sound_seq)\u001b[0m\n\u001b[1;32m    133\u001b[0m         )\n\u001b[1;32m    134\u001b[0m         sound_seq_estimated = self.sound_scaler.inverse_transform(\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0msound_seq_estimated_unscaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         )\n\u001b[1;32m    137\u001b[0m         \u001b[0msound_seq_repeated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynthesizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynthesize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mart_seq_estimated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36minverse_transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m   1020\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m             \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"allow-nan\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m         )\n\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"allow-nan\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;31m# safely to reduce dtype induced overflows.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mis_float\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m\"fc\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mis_float\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_safe_accumulator_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_float\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36m_safe_accumulator_op\u001b[0;34m(op, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m     \"\"\"\n\u001b[1;32m    892\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    894\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2259\u001b[0m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[0;32m-> 2260\u001b[0;31m                           initial=initial, where=where)\n\u001b[0m\u001b[1;32m   2261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                   if v is not np._NoValue}\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agents_ema = {}\n",
    "datasets_occlusions = {}\n",
    "\n",
    "for agent_alias, agent_path in tqdm(agents_alias.items()):\n",
    "    agent_ema = agents_ema[agent_path] = {}\n",
    "    \n",
    "    agent = ImitativeAgent.reload(agent_path)\n",
    "    synth_dataset = agent.synthesizer.dataset\n",
    "    \n",
    "    main_dataset = agent.get_main_dataset()\n",
    "    agent_features = agent.repeat_datasplit(None)\n",
    "    \n",
    "    for dataset_name, dataset_features in agent_features.items():\n",
    "        if dataset_name not in datasets_occlusions:\n",
    "            dataset = Dataset(dataset_name)\n",
    "            palate = dataset.palate\n",
    "            vowels = dataset.phones_infos[\"vowels\"]\n",
    "            datasets_lab = {dataset_name: dataset.lab}\n",
    "            datasets_ema = {dataset_name: dataset.get_items_data(\"ema\")}\n",
    "            consonants_indexes = abx_utils.get_datasets_phones_indexes(\n",
    "                datasets_lab, TONGUE_CONSONANTS, vowels\n",
    "            )\n",
    "            datasets_occlusions[dataset_name] = abx_utils.get_occlusions_indexes(\n",
    "                TONGUE_CONSONANTS, consonants_indexes, DETECTION_METHODS, datasets_ema, palate,\n",
    "            )\n",
    "        \n",
    "        items_estimated_ema = agent_ema[dataset_name] = {}\n",
    "        \n",
    "        items_estimated_art = dataset_features[\"art_estimated\"]\n",
    "        for item_name, item_estimated_art in items_estimated_art.items():\n",
    "            item_estimated_ema = synth_dataset.art_to_ema(item_estimated_art)\n",
    "            items_estimated_ema[item_name] = item_estimated_ema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "286e74fb-89f5-4446-9a44-70ab93fe95dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b1f6975f69a4973a8764a0d44829736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='dataset_name', options=('pb2007',), value='pb2007'), Output()), _d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_dataset(dataset_name):\n",
    "    dataset = Dataset(dataset_name)\n",
    "    items_ema = dataset.get_items_data(\"ema\")\n",
    "    dataset_occlusions = datasets_occlusions[dataset_name]\n",
    "    palate = dataset.palate\n",
    "    \n",
    "    display_xlim = (dataset.ema_limits[\"xmin\"] * 0.95, dataset.ema_limits[\"xmax\"] * 1.05)\n",
    "    display_ylim = (dataset.ema_limits[\"ymin\"] * 0.95, dataset.ema_limits[\"ymax\"] * 1.05)\n",
    "    \n",
    "    def show_occlusions(offset=2):\n",
    "        consonants_stats = {}\n",
    "        for consonant, occlusions in dataset_occlusions.items():\n",
    "            plt.figure(figsize=(12, 3), dpi=60)\n",
    "\n",
    "            ax_start = plt.subplot(121, aspect=\"equal\")\n",
    "            ax_start.set_title(\"%s start (PB original)\" % consonant)\n",
    "            ax_start.set_xlim(*display_xlim)\n",
    "            ax_start.set_ylim(*display_ylim)\n",
    "            ax_start.plot(palate[:, 0], palate[:, 1])\n",
    "            ax_start.set_xticks([])\n",
    "            ax_start.set_yticks([])\n",
    "\n",
    "            ax_stop = plt.subplot(122, aspect=\"equal\")\n",
    "            ax_stop.set_title(\"%s stop (PB original)\" % consonant)\n",
    "            ax_stop.set_xlim(*display_xlim)\n",
    "            ax_stop.set_ylim(*display_ylim)\n",
    "            ax_stop.plot(palate[:, 0], palate[:, 1])\n",
    "            ax_stop.set_xticks([])\n",
    "            ax_stop.set_yticks([])\n",
    "\n",
    "            occlusions_start_ema = []\n",
    "            occlusions_stop_ema = []\n",
    "            for occlusion in occlusions:\n",
    "                item_ema = items_ema[occlusion[1]]\n",
    "                occlusions_start_ema.append(item_ema[occlusion[2] - offset])\n",
    "                occlusions_stop_ema.append(item_ema[occlusion[3] + offset])\n",
    "            occlusions_start_ema = np.array(occlusions_start_ema)\n",
    "            occlusions_stop_ema = np.array(occlusions_stop_ema) \n",
    "            \n",
    "            occlusions_stats = consonants_stats[consonant] = {}\n",
    "            for occlusions_type, occlusions_ema in {\"start\": occlusions_start_ema, \"stop\": occlusions_stop_ema}.items():\n",
    "                lips_distance = np.sqrt(np.sum((occlusions_ema[:, 10:12] - occlusions_ema[:, 8:10]) ** 2, axis=1))\n",
    "                occlusions_stats[\"%s_lips\" % occlusions_type] = \"%.2f ±%.2f\" % (lips_distance.mean(), lips_distance.std())\n",
    "                \n",
    "                tongue_tip_distance = abx_utils.coil_distances_from_palate(occlusions_ema[:, 2:4], palate)\n",
    "                occlusions_stats[\"%s_tongue_tip\" % occlusions_type] = \"%.2f ±%.2f\" % (tongue_tip_distance.mean(), tongue_tip_distance.std())\n",
    "                \n",
    "                tongue_mid_distance = abx_utils.coil_distances_from_palate(occlusions_ema[:, 4:6], palate)\n",
    "                occlusions_stats[\"%s_tongue_mid\" % occlusions_type] = \"%.2f ±%.2f\" % (tongue_mid_distance.mean(), tongue_mid_distance.std())\n",
    "\n",
    "            ax_start.scatter(occlusions_start_ema[:, 0::2], occlusions_start_ema[:, 1::2], c=\"tab:blue\", s=2)\n",
    "            ax_stop.scatter(occlusions_stop_ema[:, 0::2], occlusions_stop_ema[:, 1::2], c=\"tab:blue\", s=2)\n",
    "\n",
    "            plt.subplots_adjust(wspace=-.1)\n",
    "            plt.show()\n",
    "            \n",
    "        consonants_stats = pd.DataFrame.from_dict(consonants_stats, orient=\"index\")\n",
    "        display(consonants_stats)\n",
    "    \n",
    "    ipw.interact(show_occlusions, offset=(0, 10))\n",
    "\n",
    "ipw.interactive(show_dataset, dataset_name=datasets_occlusions.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e22e4695-2f9d-4c6e-a234-bb55a677660e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08bc563d98d144fe89ef9f493d1aeb0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='agent_alias', options=('pb2007 synth_art=art_params jerk_c=0.014 j…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_agent(agent_alias):\n",
    "    agent_path = agents_alias[agent_alias]\n",
    "    agent = ImitativeAgent.reload(agent_path, load_nn=False)\n",
    "    synth_dataset = agent.synthesizer.dataset\n",
    "    palate = synth_dataset.palate\n",
    "    agent_ema = agents_ema[agent_path]\n",
    "    \n",
    "    display_xlim = (synth_dataset.ema_limits[\"xmin\"] * 0.95, synth_dataset.ema_limits[\"xmax\"] * 1.05)\n",
    "    display_ylim = (synth_dataset.ema_limits[\"ymin\"] * 0.95, synth_dataset.ema_limits[\"ymax\"] * 1.05)\n",
    "    \n",
    "    def show_occlusions(offset=2):\n",
    "        consonants_stats = {}\n",
    "    \n",
    "        for dataset_name in agent.config[\"dataset\"][\"names\"]:\n",
    "            dataset = Dataset(dataset_name)\n",
    "            items_ema = agent_ema[dataset_name]\n",
    "            dataset_occlusions = datasets_occlusions[dataset_name]\n",
    "\n",
    "            for consonant, occlusions in dataset_occlusions.items():\n",
    "                plt.figure(figsize=(12, 3), dpi=60)\n",
    "\n",
    "                ax_start = plt.subplot(121, aspect=\"equal\")\n",
    "                ax_start.set_title(\"%s start (jerk=%s)\" % (consonant, agent.config[\"training\"][\"jerk_loss_weight\"]))\n",
    "                ax_start.set_xlim(*display_xlim)\n",
    "                ax_start.set_ylim(*display_ylim)\n",
    "                ax_start.plot(palate[:, 0], palate[:, 1])\n",
    "                ax_start.set_xticks([])\n",
    "                ax_start.set_yticks([])\n",
    "\n",
    "                ax_stop = plt.subplot(122, aspect=\"equal\")\n",
    "                ax_stop.set_title(\"%s stop (jerk=%s)\" % (consonant, agent.config[\"training\"][\"jerk_loss_weight\"]))\n",
    "                ax_stop.set_xlim(*display_xlim)\n",
    "                ax_stop.set_ylim(*display_ylim)\n",
    "                ax_stop.plot(palate[:, 0], palate[:, 1])\n",
    "                ax_stop.set_xticks([])\n",
    "                ax_stop.set_yticks([])\n",
    "\n",
    "                occlusions_start_ema = []\n",
    "                occlusions_stop_ema = []\n",
    "                for occlusion in occlusions:\n",
    "                    item_ema = items_ema[occlusion[1]]\n",
    "                    occlusions_start_ema.append(item_ema[occlusion[2] - offset])\n",
    "                    occlusions_stop_ema.append(item_ema[occlusion[3] + offset])\n",
    "\n",
    "                occlusions_start_ema = np.array(occlusions_start_ema)\n",
    "                occlusions_stop_ema = np.array(occlusions_stop_ema) \n",
    "                \n",
    "                occlusions_stats = consonants_stats[consonant] = {}\n",
    "                for occlusions_type, occlusions_ema in {\"start\": occlusions_start_ema, \"stop\": occlusions_stop_ema}.items():\n",
    "                    lips_distance = np.sqrt(np.sum((occlusions_ema[:, 10:12] - occlusions_ema[:, 8:10]) ** 2, axis=1))\n",
    "                    occlusions_stats[\"%s_lips\" % occlusions_type] = \"%.2f ±%.2f\" % (lips_distance.mean(), lips_distance.std())\n",
    "\n",
    "                    tongue_tip_distance = abx_utils.coil_distances_from_palate(occlusions_ema[:, 2:4], palate)\n",
    "                    occlusions_stats[\"%s_tongue_tip\" % occlusions_type] = \"%.2f ±%.2f\" % (tongue_tip_distance.mean(), tongue_tip_distance.std())\n",
    "\n",
    "                    tongue_mid_distance = abx_utils.coil_distances_from_palate(occlusions_ema[:, 4:6], palate)\n",
    "                    occlusions_stats[\"%s_tongue_mid\" % occlusions_type] = \"%.2f ±%.2f\" % (tongue_mid_distance.mean(), tongue_mid_distance.std())\n",
    "\n",
    "                ax_start.scatter(occlusions_start_ema[:, 0::2], occlusions_start_ema[:, 1::2], c=\"tab:blue\", s=2)\n",
    "                ax_stop.scatter(occlusions_stop_ema[:, 0::2], occlusions_stop_ema[:, 1::2], c=\"tab:blue\", s=2)\n",
    "\n",
    "                plt.subplots_adjust(wspace=-.1)\n",
    "                plt.show()\n",
    "                 \n",
    "        consonants_stats = pd.DataFrame.from_dict(consonants_stats, orient=\"index\")\n",
    "        display(consonants_stats)\n",
    "    ipw.interact(show_occlusions, offset=(0, 10))\n",
    "\n",
    "ipw.interactive(show_agent, agent_alias=sorted(agents_alias.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47802e7-cd60-4116-8dd5-a21e5af237b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
