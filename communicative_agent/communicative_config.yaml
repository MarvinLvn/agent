model:
  direct_model:
    nb_derivatives: 0
    hidden_layers: [256, 256, 256, 256]
    activation: relu
    batch_norm: true
    dropout_p: 0.25
  inverse_model:
    num_layers: 2
    hidden_size: 32
    dropout_p: 0.25
    bidirectional: true
synthesizer:
  name: 1f5a2a3c7ba67a6133af746d4fc976af
sound_quantizer:
  name: 2c703da6ced1172e5a996ac08c5e127f
training:
  inverse_model_learning_rate: 0.001
  direct_model_learning_rate: 0.001
  max_epochs: 500
  patience: 25
  jerk_loss_weight: 0
